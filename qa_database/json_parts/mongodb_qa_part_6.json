{
    "dataset": "mongodb_QA_DB",
    "version": "1.0",
    "generated_for": "LLM_training_and_retrieval",
    "parts": [
        {
            "part_id": "Part_6",
            "questions": [
                {
                    "id": 51,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "What happens if a document hits the 16MB limit during an update?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "The update fails. MongoDB will send an error message saying the document is too big. No data is saved for that update, even if it was just 1 byte over the limit."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "If an operation pushes a document past 16MB, the database throws a fatal error and aborts the operation. This usually happens in applications that use 'Unbounded Arrays' (e.g., adding every user click to an array). Best practice is to use 'Bucketing' or 'Referencing' before you ever get close to that limit."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "The server returns error code `10334`. The document on disk remains in its previous state (Atomic failure). Note that the 16MB limit applies to the BSON representation, which is often larger than the JSON you see in your code. You should monitor `avgObjSize` periodically to find 'bloated' documents."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "The update operation is rejected by the server if the resulting BSON document exceeds the 16MB maximum size limit."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Trying to fit one more person into a crowded elevator'. The alarm goes off, and the doors won't close. No one moves until someone (some data) gets out, or you get a second elevator (a new collection)."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "The update is rejected, and the previous version of the document is preserved."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "While 16MB sounds small, it's actually massive for text data. However, if you store big binary strings or deep logs, you hit it. The limit is enforced at the server level to prevent a single 'Elephant Document' from saturating the network bandwidth and starving other requests during replication."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "You get a big error message and the save doesn't work. Keep your documents small!"
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "In sharded clusters, a massive document can become an 'Orphan' if it's too big to be moved between shards during balancing. This effectively 'Pins' that data to one shard, breaking your horizontal scaling logic. This is why keeping document size consistent is part of 'Database Health' monitoring."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "The failure of a write operation due to the resulting BSON document exceeding the hard 16MB limit."
                        }
                    ]
                },
                {
                    "id": 52,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "Why is `count()` sometimes inaccurate?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "`count()` (the old way) is 'Lazy'. It doesn't actually count the items; it just looks at the metadata (like a quick note on the folder). If the computer crashed lately or data is being moved between servers, that note might be wrong."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "In sharded clusters, `count()` can be inaccurate if there are 'Orphaned Documents' (leftovers from a chunk migration). For precise numbers, you must use `countDocuments()` or the `$count` aggregation stage, which performs an actual scan of the documents rather than relying on metadata."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "The metadata-based `count()` command is faster (O(1)) but potentially inconsistent. `countDocuments()` is O(n) because it iterates over the index. In a Sharded environment, secondaries might also report different counts due to replication lag or incomplete chunk balancing."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "Metadata-based counting methods in MongoDB may return imprecise results in sharded clusters or following unclean shutdowns."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Counting People at a Stadium'. Method 1: Ask the ticket booth how many tickets they sold (Metadata - Fast but maybe wrong if someone snuck in). Method 2: Actually walk through the stands and count every head (countDocuments - Slow but 100% accurate)."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "Reliance on metadata can lead to inaccuracies in distributed or unstable clusters."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "In sharding, when a chunk moves from Shard A to Shard B, it might temporarily exist in both places. The metadata-based count would add both, resulting in a number higher than truly exists. `countDocuments()` filters out these orphaned records automatically, making it the only choice for financial or inventory reports."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "Always use `countDocuments()` if you need the exact, perfect number!"
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "For performance on extremely large collections where O(n) is too slow, we recommend 'Estimated Document Count' (for the whole collection) or maintaining a manual counter using `$inc` on every insert/delete. This gives you O(1) speed with application-level accuracy guarantees."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "The potential for discrepancies between the reported number of documents and the actual number of documents due to metadata-based estimation."
                        }
                    ]
                },
                {
                    "id": 53,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "The 'In-Memory Sort' 32MB limit: What is it?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "If you try to sort a huge list without using an index, MongoDB has to load it all into its 'Brain' (RAM) to sort it. But the brain is only allowed 32MB for this. If your data is 33MB, the query crashes instantly."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "When performing a sort operation without an index, MongoDB must load all documents into memory. If the size exceeds 32MB, the operation fails with an error. To fix this, you either add an index (so the database reads the data already sorted) or use an aggregation pipeline with `allowDiskUse: true`."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "The 32MB limit applies to the 'Blocking Sort' phase. If an index scan cannot provide the sort order, MongoDB performs an 'IN-MEMORY' sort. If you use `aggregation`, you can spill to disk using `allowDiskUse`, but this is significantly slower than using a proper compound index."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "The hard limit on the amount of RAM MongoDB can use to perform a sort operation that is not supported by an index."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Alphabetizing 1,000 files on your desk'. If your desk (the 32MB RAM) is too small to spread out all the papers, you can't sort them. You either need a bigger desk or you need to have the files delivered to you already in order (the Index)."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "A memory safety cap that kills queries attempting large, non-indexed sorts."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "Compound indexes are the solution. If you want to sort by `{ age: 1, name: 1 }`, your index MUST be `{ age: 1, name: 1 }`. An index on just `{ age: 1 }` won't help with the 'name' part of the sort once you have multiple people with the same age, and you'll hit the memory limit for those specific groups."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "Indexes aren't just for finding things—they are for sorting things too!"
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "In sharded clusters, the 'Merge' phase of a sort also takes memory on the `mongos` router. While the limit is higher there, a massive 'Sort and Limit' across 100 shards can still starve the router's memory. Always use 'Covered Queries' for sorts when possible to minimize the BSON payload size during the merge."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "A restriction that prevents sort operations from consuming more than 32 megabytes of system memory."
                        }
                    ]
                },
                {
                    "id": 54,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "What is a 'Ghost Write' (or Lost Update)?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "A Ghost Write is when two people edit the same file at the same time. Person A's change gets overwritten by Person B's change, and Person A is confused because their work simply 'disappeared'."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "This happens when you 'Fetch-Modify-Save'. Client A reads doc, Client B reads doc. Client A saves change. Client B saves change (overwriting A's work). To prevent this, use **Atomic Operators** like `$set` and `$inc`, or use **Optimistic Concurrency Control** (checking a 'version' number before saving)."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "A race condition in the 'Check-and-Set' pattern. If document `_id: 1` has `v: 10`, and two threads want to add 5, they both read 10 and write back 15. The result is 15 (wrong) instead of 20. Atomic `$inc` solves this by performing the addition *inside* the single write-lock."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "A data consistency problem where concurrent updates to the same document lead to one update being overwritten and lost."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Two people trying to set the thermostat'. Person A walks up, sees 70, and wants it 72 (+2). At the same time, Person B sees 70 and wants it 75 (+5). If they both just type 'Set to 72' and 'Set to 75', the result is 75. A lost her +2 increase."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "The overwriting of data due to lack of synchronization between concurrent read-modify-write cycles."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "MongoDB handles this at the 'Document Level'. While updates to different documents happen in parallel, updates to the 'Same' document are serialized (one at a time). Using native operators like `$push` or `$bit` ensures that even with 1,000 threads, every single increment is accounted for correctly."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "Don't 'read then edit' in your code; let MongoDB do the math for you!"
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "In distributed systems, you must also consider 'Write Concern'. Even if the Primary successfully increments, if that node crashes before the change reaches Secondaries, the increment could be 'Lost' during failover. Always use `w: majority` for mission-critical counters to ensure the success is recorded on a majority of nodes."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "A scenario where a successful write operation is effectively nullified by a subsequent concurrent write."
                        }
                    ]
                },
                {
                    "id": 55,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "What are 'Jumbo Chunks' in Sharding?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "A Jumbo Chunk is a 'Unsplittable Piece of Data'. Usually, MongoDB cuts your data into small 64MB chunks. But if all your data has the *same* ID (like everyone living in the same house), MongoDB can't find a place to cut. The chunk grows until it's too heavy to move."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "Jumbo chunks occur when a shard key has low 'Cardinality'. If 10 million documents have the exact same shard key value, they must live in the same chunk. Since a chunk can't be split between multiple keys, it exceeds the 64MB limit. This prevents the 'Balancer' from moving it, causing one shard to fill up while others stay empty."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "A chunk that exceeds the `maxChunkSize` but contains only one unique shard key value. Once a chunk is marked 'Jumbo', it is excluded from migrations. This is a common pitfall when sharding by 'Status' or 'Gender'—fields with too few unique values to spread data evenly."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "Chunks in a sharded cluster that become too large to be migrated or split due to low unique value distribution in the shard key."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Packing a Shipping Container'. Usually, you put different small boxes in. But if someone gives you one 'Solid Gold Statue' that weighs 100 tons, you can't lift it. Your crane (the Balancer) breaks, and that statue is stuck in that one port forever."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "Unsplittable, unmovable data chunks caused by poorly chosen, low-cardinality shard keys."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "To fix a jumbo chunk, you often have to 'Refine' the shard key. For example, change your key from `{ country: 1 }` to `{ country: 1, userid: 1 }`. This adds a second layer of uniqueness, allowing MongoDB to find a new point to 'Cut' the data and split the jumbo chunk into smaller, movable pieces."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "Always pick a shard key with millions of possible values, like a Username or Email!"
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "In modern MongoDB (5.0+), the balancer is much more resilient. However, if a shard key is 'Monotonically Increasing' (like a Timestamp), you can still create 'Hot Jumbo Chunks' where all new data lands on one shard. While it can split, the 'Moving' of data never stops, which uses 50% of your network bandwidth just for housekeeping."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "Data chunks in a sharded collection that exceed the chunk size threshold and cannot be split."
                        }
                    ]
                },
                {
                    "id": 56,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "The 'Write Conflict' error: What causes it?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "A Write Conflict is 'A Collision'. Two people try to update the exact same document at the exact same millisecond. MongoDB has to stop one of them and say 'Whoops, someone beat you to it. Try again in a millisecond'."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "In multi-document transactions, MongoDB uses 'Optimistic Concurrency Control'. If two transactions try to modify the same document simultaneously, the second one will receive a 'Write Conflict' error. Your application MUST be designed to catch this error and retry the transaction logic."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "Error code `WriteConflict`. This typically happens in the WiredTiger engine when two sessions attempt to modify the same B-Tree leaf node. While MongoDB handles simple updates automatically with 'Internal Retries', manual 'Multi-document Transactions' return the error to the driver."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "Consistency errors that occur in MongoDB transactions when multiple operations attempt to write to the same document concurrently."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Two people trying to grab the last donut'. If both hands hit the donut at once, the baker (the Database) slaps both hands away and says 'Start over! Let's see who is faster this time'."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "Conflicts that arise when concurrent transactions attempt to modify the same data record."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "Write conflicts are a sign of 'Contention'. If you see many of these, your data model might be 'too centralized'. For example, if you have one 'GLOBAL_STATS' document that every user updates on every click, you will have extreme write conflicts. You should 'Shred' that document into many smaller ones or use a 'Log and Aggregate' approach."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "Don't worry, it's normal! Just tell your code to Wait and Try Again."
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "The `TransientTransactionError` label is attached to these in the driver. Modern drivers have a `withTransaction` helper that automatically handles the retry logic for you. However, you must ensure your transaction code is 'Pure' (no side effects like sending an email) because it might run 2 or 3 times before succeeding."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "An error returned by MongoDB when concurrent write operations on the same data resource cannot be resolved."
                        }
                    ]
                },
                {
                    "id": 57,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "Why is the `$where` operator dangerous?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "Using `$where` is 'Running custom code inside the database'. It's very slow because the database has to stop being a database and start being a JavaScript runner. It also lets hackers potentially 'Injection' bad code into your server."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "`$where` allows you to pass a JavaScript string or function for evaluation. It's dangerous for two reasons: 1. **Performance**: It cannot use indexes and is incredibly slow. 2. **Security**: It opens you up to 'NoSQL Injection' if you pass untrusted user input into the JS string."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "Forces the scan of every document. For each document, BSON must be converted to a JavaScript object, the script executed, and the result returned. This process is single-threaded in many older versions and significantly increases CPU load and latency. Always prefer the 'Aggregation Framework' for complex logic."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "A MongoDB operator that executes JavaScript logic for query filtering, often causing significant performance and security issues."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Hiring a translator for every word you read'. Instead of just looking for the word 'Apple', you ask the translator: 'Is this word delicious?'. The translator has to think, look it up, and answer. It's 1,000x slower than just recognizing the letters yourself."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "A slow, non-indexed JavaScript-based query operator with high security risks."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "In many cases, users use `$where` to compare two fields in the same document (e.g., `price > wholesalePrice`). Since MongoDB 3.6, you can use the `$expr` operator in a normal `find()` query to do this using native aggregation syntax, which is much safer and significantly faster than `$where`."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "Avoid `$where` like the plague! Use standard filters instead."
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "If you absolutely MUST use it, you can disable it at the server level using `--noscripting`. This is a common security 'Hardening' step for enterprise MongoDB clusters to ensure that no developer (or attacker) can run arbitrary code on the database hardware."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "An operator that passes a string containing a JavaScript expression or a full JavaScript function to the query system."
                        }
                    ]
                },
                {
                    "id": 58,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "What is 'Stale Read' and how does Read Preference cause it?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "A Stale Read is 'Reading Old News'. If you save a photo on the Main server, but your app reads it from a Backup server (Secondary) that hasn't seen the update yet, the photo 'disappears'. This happens because it takes a few milliseconds for backup servers to catch up."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "When you use `ReadPreference: secondary`, you allow your app to read from non-primary nodes for better scaling. However, because replication is 'Asynchronous', those nodes might be a few milliseconds (or seconds) behind. This usually causes 'Confusion' where a user updates their profile, clicks 'Refresh', and sees their old info."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "Caused by 'Replication Lag'. To prevent this, you should use 'Causal Consistency' in your driver sessions. This ensures that a read following a write will always see the effects of that write, even if the read is directed to a secondary node."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "A scenario where a query returns outdated data because it was executed on a secondary node that has not yet applied all updates from the primary."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Telling a secret to a group'. If you tell the first person (Primary), and then immediately ask the 5th person in line (Secondary), they might not know the secret yet. Information isn't instant; it has a 'Travel Time'."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "Retrieving outdated data from lagged replica set secondaries."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "By default, MongoDB uses `ReadPreference: primary`, so you never see stale data. Developers usually turn on 'Secondary Reads' to save money or handle massive traffic. If you do this, you must accept 'Eventual Consistency'. If your app can't handle a user seeing old data for 200ms, keep them on the Primary."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "Only read from backups if speed is more important than perfect accuracy!"
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "You can monitor `replicationLag` in Atlas or using `db.printSecondaryReplicationInfo()`. If the lag is too high, you can use 'Hedged Reads' (version 5.0+) which queries multiple nodes and takes the first response. This doesn't prevent staleness, but it reduces the chance of 'Extremely' stale reads by avoiding slow nodes."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "The condition of reading data that does not reflect the most recent write operations."
                        }
                    ]
                },
                {
                    "id": 59,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "How can 'Unindexed Updates' lock your database?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "If you try to update an item without using an index, the database has to look at every single record to find the right one. While it's looking, it might 'Lock' the collection, making everyone else wait. If you have 10 million items, this can freeze your whole app for minutes."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "An update requires a 'Find' phase. If that find isn't indexed, it results in a 'Collscan'. MongoDB locks the document (or collection in older versions) while it tries to match documents. With high concurrency, these slow scans pile up, exhausting the connection pool and effectively DOSing your own database."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "Uses an Intent-Exclusive (IX) lock. While WiredTiger has document-level locking, a slow unindexed update spans thousands of documents, essentially 'Holding' resources. If many such updates run, the 'Metadata Lock' of the collection can become a bottleneck, preventing even simple reads from finishing."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "Database performance degradation caused by search-and-modify operations that lack supporting indexes, leading to excessive resource consumption."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Asking a librarian to change the price on every book with a blue cover' in a library without a color-catalog. The librarian has to touch every single book in the building. While she's doing that, she can't help anyone find anything else."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "Extreme resource contention caused by full-collection scans during update operations."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "Monitoring `slowms` is vital here. Any query longer than 100ms is logged. If you see many 'update' operations in your logs with `planSummary: COLLSCAN`, you have a critical indexing gap. Adding a simple single-field index will usually drop the update time from 5 seconds to 5 milliseconds."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "Always make sure the 'query' part of your update has a proper index!"
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "In sharded clusters, an unindexed update must be broadcast to EVERY shard (`broadcast: true`). This multiplies the impact across your whole cluster. This is why many enterprise DBAs enforce a 'No Unindexed Queries' policy at the driver level using `setParameter: notablescan`."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "The exhaustion of database resources and increase in latency caused by non-performant write operations."
                        }
                    ]
                },
                {
                    "id": 60,
                    "topic": "Edge Cases & Pitfalls",
                    "difficulty": "Advanced",
                    "question": "What is 'Index Intersection' and its drawback?",
                    "answer_variants": [
                        {
                            "variant_id": 1,
                            "style": "simple",
                            "answer": "Index Intersection is 'Using two different maps at once'. If you search for 'Red' and 'Large', MongoDB might try to use the 'Color' index and the 'Size' index together. It sounds smart, but it's often slower than just having one combined 'Color-Size' map."
                        },
                        {
                            "variant_id": 2,
                            "style": "interview",
                            "answer": "Index intersection occurs when MongoDB uses more than one index to fulfill a query. While it allows for flexibility (you don't need a compound index for every combination), it is mathematically slower than a single **Compound Index**. The 'Merging' of the two index results takes substantial CPU and RAM."
                        },
                        {
                            "variant_id": 3,
                            "style": "technical",
                            "answer": "A query plan involving `AND_SORTED` or `AND_HASH`. The optimizer scans two indexes, finds the common document IDs, and then fetches them. This is often outperformed by a compound index because the compound index filters everything in a single, sorted traversal."
                        },
                        {
                            "variant_id": 4,
                            "style": "exam",
                            "answer": "The use of multiple single-field indexes to satisfy a multi-criteria query, typically as a fallback when a compound index is unavailable."
                        },
                        {
                            "variant_id": 5,
                            "style": "analogy",
                            "answer": "It's like 'Searching for a specific Doctor'. You look at a list of 'Cardiologists' and a separate list of 'Spanish Speakers'. You have to cross-check both lists names by name to find someone on both. It's much faster to just have a list that says 'Spanish-Speaking Cardiologists' from the start."
                        },
                        {
                            "variant_id": 6,
                            "style": "one_liner",
                            "answer": "Combining two indexes for one query, usually less efficient than a single compound index."
                        },
                        {
                            "variant_id": 7,
                            "style": "deep_explanation",
                            "answer": "You can detect this in `.explain()` by looking for an `INTERSECT` stage. If you see it, it's a hint that you should probably build a compound index on those two fields. Compound indexes are the #1 way to scale high-concurrency MongoDB applications."
                        },
                        {
                            "variant_id": 8,
                            "style": "beginner_friendly",
                            "answer": "Don't rely on multiple small indexes; build one 'Smart' index per common query!"
                        },
                        {
                            "variant_id": 9,
                            "style": "advanced",
                            "answer": "Intersection only works with specific operators and can't be used for 'Sorting'. If you have an intersection but your query also includes a `.sort()`, the intersection will likely be abandoned in favor of a single index scan + a heavy 'In-Memory Sort' (32MB pitfall!). Compound indexes handle both filtering and sorting in one go."
                        },
                        {
                            "variant_id": 10,
                            "style": "strict_definition",
                            "answer": "A technique where MongoDB uses multiple indexes to fulfill a query and then intersects the results."
                        }
                    ]
                }
            ]
        }
    ]
}