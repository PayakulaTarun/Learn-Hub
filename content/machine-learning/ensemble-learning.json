{
  "title": "Ensemble Learning",
  "slug": "ensemble-learning",
  "subject": "Machine Learning",
  "category": "Artificial Intelligence",
  "level": "Intermediate",
  "estimated_read_time": "30 mins",
  "prerequisites": [
    "Random Forest"
  ],
  "learning_objectives": [
    "Bagging",
    "Boosting",
    "Stacking",
    "Voting"
  ],
  "theory": "Combining models. \n- **Bagging**: Parallel (Random Forest).\n- **Boosting**: Sequential. Each model corrects errors of previous (AdaBoost, XGBoost).\n- **Stacking**: Meta-model learns from output of base models.",
  "syntax": "VotingClassifier",
  "examples": [],
  "common_mistakes": [],
  "interview_questions": [
    {
      "question": "Bagging vs Boosting?",
      "answer": "Bagging reduces Variance (Overfitting). Boosting reduces Bias (Underfitting).",
      "difficulty": "Medium"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [],
  "exam_notes": [
    "XGBoost wins Kaggle competitions."
  ],
  "summary": "Dream team."
}