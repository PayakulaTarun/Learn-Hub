{
  "title": "Naive Bayes",
  "slug": "naive-bayes",
  "subject": "Machine Learning",
  "category": "Artificial Intelligence",
  "level": "Intermediate",
  "estimated_read_time": "30 mins",
  "prerequisites": [
    "Probability Basics for AI"
  ],
  "learning_objectives": [
    "Bayes Theorem",
    "Independence Assumption",
    "Gaussian vs Multinomial"
  ],
  "theory": "Probabilistic Classifier. \nNaive Assumption: Features are INDEPENDENT. \nFast, simplistic, but works surprisingly well for text.",
  "syntax": "GaussianNB()",
  "examples": [
    {
      "code": "P(Spam|Word) = P(Word|Spam)P(Spam) / P(Word)",
      "output": "Probability",
      "explanation": "Text mining staple."
    }
  ],
  "common_mistakes": [],
  "interview_questions": [
    {
      "question": "Zero Frequency Problem?",
      "answer": "If a word is distinct to training data, probability becomes 0. Solved by Laplace Smoothing (Add 1).",
      "difficulty": "Advanced"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [
    {
      "scenario": "Sentiment Analysis",
      "description": "Counting word probabilities.",
      "code": "NLP"
    }
  ],
  "exam_notes": [],
  "summary": "Naive but effective.",
  "order": 17
}