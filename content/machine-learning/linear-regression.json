{
  "title": "Linear Regression",
  "slug": "linear-regression",
  "subject": "Machine Learning",
  "category": "Artificial Intelligence",
  "level": "Intermediate",
  "estimated_read_time": "30 mins",
  "prerequisites": [
    "Regression vs Classification"
  ],
  "learning_objectives": [
    "Best Fit Line",
    "Slope & Intercept",
    "Gradient Descent",
    "OLS"
  ],
  "theory": "Simple Regression. Fitting a straight line `y = mx + c`. \nGoal: Minimize Sum of Squared Errors (Residuals). \nSolved via OLS (Math formula) or Gradient Descent (Iterative optimization).",
  "syntax": "LinearRegression()",
  "examples": [
    {
      "code": "y = 2x + 5. \nIf x=10, y=25.",
      "output": "Prediction",
      "explanation": "Linear relationship."
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Using on Non-linear data",
      "correction": "Linear Regression fails on curves. Use Polynomial or Trees.",
      "example": "U-Shape data."
    }
  ],
  "interview_questions": [
    {
      "question": "What are assumptions of Linear Regression?",
      "answer": "Linearity, Homoscedasticity (Constant variance of errors), No Multicollinearity, Normality of errors.",
      "difficulty": "Medium"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [],
  "exam_notes": [
    "Gradient Descent update rule."
  ],
  "summary": "The grandfather of ML."
}