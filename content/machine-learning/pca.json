{
  "title": "Principal Component Analysis (PCA)",
  "slug": "pca",
  "subject": "Machine Learning",
  "category": "Artificial Intelligence",
  "level": "Intermediate",
  "estimated_read_time": "35 mins",
  "prerequisites": [
    "Dimensionality Reduction"
  ],
  "learning_objectives": [
    "Variance Maximization",
    "Eigenvectors",
    "Eigenvalues",
    "Scree Plot"
  ],
  "theory": "Linear check transformation. \nFinds new axes (Principal Components) that explain maximum variance. \nFirst PC explains most variance, Second explains next most orthogonal variance, etc.",
  "syntax": "PCA(n_components=2)",
  "examples": [
    {
      "code": "100 features -> PCA -> 5 components explaining 95% variance.",
      "output": "Efficiency",
      "explanation": "Speeds up training."
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Forgetting Scaling",
      "correction": "PCA maximizes variance. If one feature has huge scale (Salary), it becomes PC1 purely due to scale.",
      "example": "Scale first."
    }
  ],
  "interview_questions": [
    {
      "question": "Are PCA components interpretable?",
      "answer": "Not usually. They are linear combinations of original features.",
      "difficulty": "Medium"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [
    {
      "scenario": "Image Compression",
      "description": "Sending fewer values to reconstruct image.",
      "code": "Media"
    }
  ],
  "exam_notes": [
    "Unsupervised technique."
  ],
  "summary": "Rotating the world."
}