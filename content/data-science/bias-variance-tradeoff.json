{
  "title": "Bias-Variance Tradeoff",
  "slug": "bias-variance-tradeoff",
  "subject": "Data Science",
  "category": "Data & Analytics",
  "level": "Intermediate",
  "estimated_read_time": "20 mins",
  "prerequisites": [
    "Model Evaluation Metrics"
  ],
  "learning_objectives": [
    "Underfitting (High Bias)",
    "Overfitting (High Variance)",
    "Optimal Complexity"
  ],
  "theory": "The central problem in Supervised Learning.\n- **High Bias**: Model is too simple (Underfits). Misses relations.\n- **High Variance**: Model is too complex (Overfits). Memorizes noise.\nGoal: Balance.",
  "syntax": "N/A",
  "examples": [
    {
      "code": "Train Score: 99%, Test Score: 60% -> Overfitting\nTrain Score: 60%, Test Score: 60% -> Underfitting",
      "output": "Diagnostics",
      "explanation": "Large gap indicates variance."
    }
  ],
  "common_mistakes": [],
  "interview_questions": [
    {
      "question": "How to fix High Variance?",
      "answer": "Get more data, reduce model complexity (fewer features, prune trees), or use regularization.",
      "difficulty": "Medium"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [],
  "exam_notes": [
    "Total Error = Bias^2 + Variance + Irreducible Error."
  ],
  "summary": "Every model tuning decision is a navigation of this tradeoff.",
  "order": 46
}