{
  "title": "Clustering Algorithms",
  "slug": "clustering-algorithms",
  "subject": "Data Science",
  "category": "Data & Analytics",
  "level": "Advanced",
  "estimated_read_time": "30 mins",
  "prerequisites": [
    "Unsupervised Learning"
  ],
  "learning_objectives": [
    "K-Means",
    "Hierarchical Clustering",
    "DBSCAN"
  ],
  "theory": "**K-Means**: Partitions data into K circles (centroids). Fast.\n**Hierarchical**: Builds a tree of clusters (Dendrogram). Good for small data.\n**DBSCAN**: Density-based. Finds arbitrary shapes and outliers.",
  "syntax": "KMeans(n_clusters=3).fit(X)",
  "examples": [
    {
      "code": "kmeans.labels_",
      "output": "[0, 1, 0, ...]",
      "explanation": "Assigns a cluster ID to each sample."
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Using K-Means on non-globular data",
      "correction": "K-Means assumes spherical clusters. It fails on 'moon' shapes. Use DBSCAN there.",
      "example": "Two interlocking half-moons."
    }
  ],
  "interview_questions": [
    {
      "question": "How to choose K in K-Means?",
      "answer": "Elbow Method (look for bend in SSE plot) or Silhouette Score.",
      "difficulty": "Medium"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [],
  "exam_notes": [
    "K-Means sensitive to outliers.",
    "DBSCAN detects noise."
  ],
  "summary": "Grouping data is one of the most primal forms of intelligence.",
  "order": 49
}