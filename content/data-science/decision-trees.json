{
  "title": "Decision Trees",
  "slug": "decision-trees",
  "subject": "Data Science",
  "category": "Data & Analytics",
  "level": "Intermediate",
  "estimated_read_time": "30 mins",
  "prerequisites": [
    "Supervised Learning"
  ],
  "learning_objectives": [
    "Entropy",
    "Information Gain",
    "Gini Impurity",
    "Pruning"
  ],
  "theory": "Splits data recursively based on features to create homogeneous nodes. It creates a flowchart-like structure. interpretable.",
  "syntax": "DecisionTreeClassifier().fit(X, y)",
  "examples": [
    {
      "code": "from sklearn import tree\ntree.plot_tree(model)",
      "output": "Tree Diagram",
      "explanation": "Visualizing the splits."
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Overfitting",
      "correction": "Trees can memorize data easily. Limit depth (`max_depth`) or prune them.",
      "example": "100% training accuracy, 50% test."
    }
  ],
  "interview_questions": [
    {
      "question": "What is Entropy?",
      "answer": "A measure of disorder or impurity. 0 means all elements belong to the same class. High entropy means mixed classes.",
      "difficulty": "Medium"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [
    {
      "scenario": "Medical Diagnosis",
      "description": "Doctors need interpretable models. If Symptom A > X and B < Y, then Disease Z.",
      "code": "Interpretability"
    }
  ],
  "exam_notes": [
    "Gini is default in sklearn.",
    "Greedy algorithm."
  ],
  "summary": "The building block for powerful ensemble methods."
}