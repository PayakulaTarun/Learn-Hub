{
  "title": "Model Evaluation Metrics",
  "slug": "model-evaluation",
  "subject": "Data Science",
  "category": "Data & Analytics",
  "level": "Intermediate",
  "estimated_read_time": "25 mins",
  "prerequisites": [
    "Supervised Learning"
  ],
  "learning_objectives": [
    "Accuracy",
    "Precision/Recall",
    "F1 Score",
    "ROC-AUC",
    "RMSE",
    "MAE"
  ],
  "theory": "How good is the model? \n- **Accuracy**: Correct/Total (Bad for imbalanced data).\n- **Precision**: How many predicted positives are real?\n- **Recall**: How many actual positives did we catch?\n- **RMSE**: Root Mean Squared Error (Regression).",
  "syntax": "metrics.classification_report(y_true, y_pred)",
  "examples": [],
  "common_mistakes": [
    {
      "mistake": "Using Accuracy for Fraud Detection",
      "correction": "99.9% transactions are legit. a model predicting 'All Legit' has 99.9% accuracy but misses all fraud. Use Recall/F1.",
      "example": "Imbalanced classes."
    }
  ],
  "interview_questions": [
    {
      "question": "What is the trade-off between Precision and Recall?",
      "answer": "Increasing threshold increases Precision but decreases Recall, and vice versa.",
      "difficulty": "Medium"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [],
  "exam_notes": [
    "Confusion Matrix.",
    "AUC=0.5 is random guessing."
  ],
  "summary": "You can't improve what you can't measure correctly."
}