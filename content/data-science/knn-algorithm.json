{
  "title": "K-Nearest Neighbors",
  "slug": "knn-algorithm",
  "subject": "Data Science",
  "category": "Data & Analytics",
  "level": "Intermediate",
  "estimated_read_time": "25 mins",
  "prerequisites": [
    "Feature Scaling"
  ],
  "learning_objectives": [
    "Distance Metrics (Euclidean)",
    "Choosing K",
    "Lazy Learning"
  ],
  "theory": "Instance-based learning. To predict a new point, look at its 'K' nearest neighbors in the training set and take the majority vote (Classification) or average (Regression).",
  "syntax": "KNeighborsClassifier(n_neighbors=5)",
  "examples": [],
  "common_mistakes": [
    {
      "mistake": "Forgetting to Scale Data",
      "correction": "KNN computes distances. If one feature has range 0-10000 and another 0-1, the first dominates. Scaling is mandatory.",
      "example": "Salary vs Age."
    }
  ],
  "interview_questions": [
    {
      "question": "What happens if K is too small or too large?",
      "answer": "K=1: Overfitting (sensitive to noise). K=N: Underfitting (predicts majority class always).",
      "difficulty": "Medium"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [
    {
      "scenario": "Recommender Systems",
      "description": "Find users similar to you (neighbors) and recommend what they liked.",
      "code": "Collaborative Filtering"
    }
  ],
  "exam_notes": [
    "Non-parametric.",
    "Expensive at inference time."
  ],
  "summary": "Simple, intuitive, but slow on large datasets.",
  "order": 30
}