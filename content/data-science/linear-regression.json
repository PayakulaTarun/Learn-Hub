{
  "title": "Linear Regression",
  "slug": "data-science-linear-regression",
  "subject": "Data Science",
  "category": "Data & Analytics",
  "level": "Intermediate",
  "estimated_read_time": "25 mins",
  "prerequisites": [
    "Supervised Learning"
  ],
  "learning_objectives": [
    "Simple Linear Regression",
    "Multiple Linear Regression",
    "Cost Function (MSE)",
    "Gradient Descent"
  ],
  "theory": "The simplest regression model. Fits a straight line `y = mx + c` (or `y = b0 + b1x1 + ...`) to minimize the error between predicted and actual values.",
  "syntax": "LinearRegression().fit(X, y)",
  "examples": [
    {
      "code": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(model.coef_)",
      "output": "Coefficients",
      "explanation": "Slopes for each feature."
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Assumptions violation",
      "correction": "Linear Regression assumes linearity, homoscedasticity, independence, and normality of errors. Check residuals plot.",
      "example": "Fitting a curve with a line."
    }
  ],
  "interview_questions": [
    {
      "question": "What is R-squared?",
      "answer": "Coefficient of determination. Proportion of variance in the dependent variable explained by the independent variables. 1 is perfect.",
      "difficulty": "Medium"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [
    {
      "scenario": "Housing Prices",
      "description": "Predict price based on square footage, rooms, and location.",
      "code": "Regression"
    }
  ],
  "exam_notes": [
    "OLS (Ordinary Least Squares).",
    "Minimizes Sum of Squared Errors."
  ],
  "summary": "The 'Hello World' of Machine Learning models.",
  "order": 43
}