{
  "title": "Ensemble Learning",
  "slug": "ensemble-learning",
  "subject": "Data Science",
  "category": "Data & Analytics",
  "level": "Intermediate",
  "estimated_read_time": "30 mins",
  "prerequisites": [
    "Decision Trees"
  ],
  "learning_objectives": [
    "Bagging (Random Forest)",
    "Boosting (Gradient Boosting, XGBoost)",
    "Stacking"
  ],
  "theory": "Combining multiple weak models (learners) to create a strong model. \n- **Bagging**: Parallel trees (Vote).\n- **Boosting**: Sequential trees (Correct errors).",
  "syntax": "RandomForestClassifier()\nXGBClassifier()",
  "examples": [],
  "common_mistakes": [],
  "interview_questions": [
    {
      "question": "Why does Random Forest reduce variance?",
      "answer": "By averaging multiple decorrelated trees (trained on bootstrap samples with random feature subsets), individual errors cancel out.",
      "difficulty": "Advanced"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [
    {
      "scenario": "Kaggle Competitions",
      "description": "XGBoost/LightGBM dominate structured data competitions.",
      "code": "Boosting"
    }
  ],
  "exam_notes": [
    "RF = Low Variance.",
    "DT = High Variance.",
    "Boosting = Low Bias."
  ],
  "summary": "Ensembles are the state-of-the-art for tabular data."
}