{
  "title": "Logistic Regression",
  "slug": "data-science-logistic-regression",
  "subject": "Data Science",
  "category": "Data & Analytics",
  "level": "Intermediate",
  "estimated_read_time": "30 mins",
  "prerequisites": [
    "Linear Regression"
  ],
  "learning_objectives": [
    "Sigmoid Function",
    "Binary Classification",
    "Log Loss",
    "Decision Boundary"
  ],
  "theory": "Unlike its name, it's for **Classification**. It squeezes the output of linear regression into a range [0, 1] using the Sigmoid function, determining the probability of a class.",
  "syntax": "LogisticRegression().fit(X, y)",
  "examples": [
    {
      "code": "prob = model.predict_proba(X_test)\npred = model.predict(X_test)",
      "output": "0 or 1",
      "explanation": "Default threshold is 0.5."
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Using it for non-linear boundaries",
      "correction": "Logistic regression produces a linear decision boundary. It cannot solve XOR problem without feature engineering.",
      "example": "N/A"
    }
  ],
  "interview_questions": [
    {
      "question": "Why use Log Loss instead of MSE for Logistic Regression?",
      "answer": "In classification, MSE results in a non-convex cost function with multiple local minima. Log Loss is convex.",
      "difficulty": "Advanced"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [
    {
      "scenario": "Spam Detection",
      "description": "Input: Email words. Output: Probability of Spam.",
      "code": "Binary Class"
    }
  ],
  "exam_notes": [
    "Sigmoid: 1 / (1 + e^-z).",
    "Output is probability."
  ],
  "summary": "The baseline model for binary classification tasks.",
  "order": 44
}