{
  "title": "Dimensionality Reduction (PCA)",
  "slug": "pca-analysis",
  "subject": "Data Science",
  "category": "Data & Analytics",
  "level": "Advanced",
  "estimated_read_time": "30 mins",
  "prerequisites": [
    "Unsupervised Learning"
  ],
  "learning_objectives": [
    "Curse of Dimensionality",
    "Principal Component Analysis",
    "Variance Explained"
  ],
  "theory": "Too many features lead to overfitting and slow training. PCA projects data onto lower dimensions (Principal Components) while preserving maximum variance (information).",
  "syntax": "PCA(n_components=2).fit_transform(X)",
  "examples": [
    {
      "code": "print(pca.explained_variance_ratio_)",
      "output": "[0.80, 0.10]",
      "explanation": "First component explains 80% of data variance."
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Applying PCA without scaling",
      "correction": "PCA maximizes variance. If one feature has huge variance (salary) vs age, it dominates. Always Standardize first.",
      "example": "Scale dependent."
    }
  ],
  "interview_questions": [
    {
      "question": "Does PCA lose information?",
      "answer": "Yes, by dropping components you lose the variance they held. You trade information for simplicity/speed.",
      "difficulty": "Easy"
    }
  ],
  "practice_problems": [],
  "real_world_use_cases": [
    {
      "scenario": "Visualization",
      "description": "Visualizing 50-dimensional gene data in 2D to see clusters.",
      "code": "2D Plot"
    }
  ],
  "exam_notes": [
    "Orthogonal components.",
    "Linear transformation."
  ],
  "summary": "PCA simplifies complexity, allowing us to see the forest instead of individual trees.",
  "order": 50
}