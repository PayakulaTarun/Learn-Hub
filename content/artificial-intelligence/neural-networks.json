{
  "title": "Neural Networks Fundamentals",
  "slug": "neural-networks",
  "subject": "Artificial Intelligence",
  "category": "Core Computer Science",
  "level": "Advanced",
  "estimated_read_time": "35 mins",
  "prerequisites": [
    "Deep Learning Overview"
  ],
  "learning_objectives": [
    "Perceptron",
    "Activation Function",
    "Loss Function",
    "Optimizer"
  ],
  "theory": "- **Perceptron**: Logical gate. \n- **Activation (ReLU/Sigmoid)**: Adds non-linearity (crucial).\n- **Loss (MSE/CrossEntropy)**: Measure error.\n- **Optimizer (SGD)**: Update weights to minimize error.",
  "syntax": "w_new = w - lr * gradient",
  "examples": [],
  "common_mistakes": [
    {
      "mistake": "Using Linear Activation",
      "correction": "A deep network with only linear activation is just a single linear regression. Use ReLU.",
      "example": "N/A"
    }
  ],
  "interview_questions": [],
  "practice_problems": [],
  "real_world_use_cases": [],
  "exam_notes": [
    "Vanishing Gradient Problem."
  ],
  "summary": "Building blocks of Deep Learning."
}